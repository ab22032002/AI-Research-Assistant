# AI Research Assistant â€” Lightweight RAG with Flan-T5 & FAISS

### Overview
This project demonstrates a lightweight Retrieval-Augmented Generation (RAG) pipeline 
built using open-source models. It uses Hugging Faceâ€™s `Flan-T5` for generation and 
`FAISS` for semantic search, allowing local private document querying without APIs.

###  Tech Stack
- **LLM:** Flan-T5 (open-source)
- **Embeddings:** Sentence Transformers (all-MiniLM-L6-v2)
- **Vector DB:** FAISS
- **Framework:** LangChain (for orchestration)
- **Fine-tuning:** LoRA (optional module)

### File Structure: 
```graphql
ğŸ“¦ ai_research_assistant/
â”‚
â”œâ”€â”€ ğŸ“ data/                         
â”‚   â”œâ”€â”€ sample_1.txt
â”‚   â””â”€â”€ sample_2.txt
â”‚
â”œâ”€â”€ ğŸ“ modules/                      
â”‚   â”œâ”€â”€ ingestion.py                 
â”‚   â”œâ”€â”€ chunking.py                  
â”‚   â”œâ”€â”€ embedding_store.py           
â”‚   â”œâ”€â”€ retriever.py                 
â”‚   â”œâ”€â”€ generator.py                 
â”‚   â””â”€â”€ utils.py                     
â”œâ”€â”€ main.py                          
â”œâ”€â”€ requirements.txt                 
â”œâ”€â”€ README.md                        
â””â”€â”€ .gitignore                       
```


###  Modules
- `ingestion.py` â€” Load and clean raw documents
- `chunking.py` â€” Split long text into semantic chunks
- `embedding_store.py` â€” Convert chunks to embeddings and store in FAISS
- `retriever.py` â€” Retrieve top-k chunks per query
- `generator.py` â€” Generate context-aware answers with Flan-T5
- `main.py` â€” Runs the end-to-end RAG pipeline

###  Design Choices
- Chunk size: 500 tokens
- Overlap: 100
- Top-k retrieval: 5
- Embedding model: `all-MiniLM-L6-v2`
- Temperature: 0.2
- Model: `google/flan-t5-base`

### How to run
```bash
pip install -r requirements.txt
python main.py
```
``` 
Upload Your Text Documents into /data/, run the pipeline, and aks the Query!
```